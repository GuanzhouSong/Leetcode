* Map
    * processes are sequentially and independently 
    * Could run multiple Map process parallelly 
    * called only once for the all input 

![pic](https://cloud.githubusercontent.com/assets/9062406/8224568/acd7638e-153d-11e5-8679-c29ee8b639f8.png)

* Reduce
    * Reduces process batches of values associated with the same key while Maps process keys independently
    * reduce called one time for the one key 

![pic](https://cloud.githubusercontent.com/assets/9062406/8224561/9744f64e-153d-11e5-81ef-fdcab0f66b1a.png)

* Different Product
    * Sort
    * Advantage:
      * A Map can know which contiguous sets of its key-value pairs (sharing the same key) need to be sent to one Reduce
      * A Reduce task can call the Reduce function on a contiguous set of key-value pairs (sharing the same key)
      * Sorting as an application becomes very simple to write

* [Implement](https://github.com/UmassJin/Leetcode/blob/master/Design/OS_concepts/Design_Map_Reduce.py)

* Some applications of MapReduce
      * Distributed Grep 
      * sorting

* Programming MapReduce
![pic](https://cloud.githubusercontent.com/assets/9062406/8266036/602361ce-16cc-11e5-96d5-c904b4b694a7.png)
![pic](https://cloud.githubusercontent.com/assets/9062406/8266038/7b1fe20e-16cc-11e5-943e-ba683e4e534c.png)
      * 1. Sometimes will assign the Map tasks to the server which is close by for the low network overhead 
      * 2. Each Reduce server will be assign the key range, so independent with each other 
      * 3. There is no communication between Maps and no communication between Reduces 
      * 4. Ensure that no Reduce starts before all Maps are finished, that is, ensure the barrier between the Map phase and Reduce phase 
      * 5. Someone writes a variant of Mapreduce without a barrier, i.e., where Reduces can start before all Maps are done computing. Which of the following are TRUE statements?
         * 1) A Mapreduce run may be incorrect since some of the key-value pairs generated by Maps may never be processed by a Reduce if the reduce function is called exactly once per key
         * 2) All Mapreduce runs could be correct if Reduces maintain partial results for keys, and update these as new key-value pairs come in
      * 6. Implement the Storage of Map and Reduce, the Map input and Reduce output will all stored Distributed File System, these DFS will typically in servers where the Map tasks and Reduce tasks are run
      * 7. These file systems will store multiple replicas for the input data block, typlically multiple the input files for 3 times in 3 different servers, when the Map tasks start, it needs to fetch the input data block from whatever the server store it correctly, it fast if the Map task and DFS in the same server
      * 8. The output of the Map task will save in local file system, this shuffle between Map task and Reduce task need fast and do not visiable for users, you do not want to overhead the DFS, which has duplicate in different, you only want the output of Map task as fast as possible.

![pic](https://cloud.githubusercontent.com/assets/9062406/8268024/90888042-172b-11e5-92d0-5b333ddff6d9.png)

* Resource Scheduling 
* YARN (Yet Another Resource Negotiator)
      * Treats each server as a collection of containers
         * Container = some CPU + some memory
         * For example for each server, we have 4 cores and 4G RAM, and each container has 1 core and 1G RAM, so that server has 4 containers and each container could have one task 
      * Has 3 main components
         * Global Resource Manager (RM)
            * Scheduling
         * Per-server Node Manager (NM)
            * Daemon and server-specific functions
            * responsble for monitoring the failure tasks in that particular server 
         * Per-application (job) Application Master (AM)
            * Container negotiation with RM and NMs
            * Detecting task failures of that job

![pic](https://cloud.githubusercontent.com/assets/9062406/8268074/387be0aa-172c-11e5-86d9-1e50e82bfa97.png)       
      
* Fault Tolerance
      * Server Failure 
         * Server running the Node Manager, tasks, one of the server running the Resource Manager, and Application Master also running in the server 
         * NM send hearbeats to RM
            * when the server fails, RM lets all affected AMs know in the failure Node(server), and reschedule all the tasks in that Node
         * NM keeps track of each task running at its server
            * If task fails while in-progress, mark the task as idle and restart it
         * AM heartbeats to RM
            * On failure, RM restarts AM, which then syncs up with its running tasks
      
      * RM failure
         * Use old checkpoints and bring up secondary RM
      
      * Heartbeats also used to piggyback container requests
         * Avoid extra messages 
   
   * Slow server
      * the slowes machine slows the entire job, (maybe the CPU issue, Bad Disk, Network Bandwidth or memory)
         * since all the Map tasks need to finished before the Reduce tasks
      * keep track of "progress" of each task (% done)
      * perform backup (replicated) execution of straggler task: task considered done when first replica complete
         called Speculative Execution

   ![pic](https://cloud.githubusercontent.com/assets/9062406/8268174/7b07aae0-1730-11e5-8ceb-3bf483252413.png)
  
      
